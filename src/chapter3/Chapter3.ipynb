{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# Coding Attention Mechanism",
   "id": "7ddff7e3c6d05051"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Simple Self Attention\n",
    "\n",
    "This code implements a simple self attention mechanism without trainable weights. Its just done to illustrate the concept of self attention.\n",
    "\n",
    "![title](./SelfAttention.png)\n",
    "\n",
    "The above image summrizes how this self attention works.\n",
    "\n",
    "Consider the following input embeddings for the tokens\n"
   ],
   "id": "9ee231f14fec5bf1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T05:06:17.707318Z",
     "start_time": "2025-08-21T05:06:15.305248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]  # step     (x^6)\n",
    "  ])\n"
   ],
   "id": "9cc6d3768fc7a100",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T05:06:21.339275Z",
     "start_time": "2025-08-21T05:06:21.334970Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "Now since we want to find Z<sub>(2)</sub> which is the context vector of the second token X<sub>(2)</sub>, we will need have our query vector as X<sub>(2)</sub>.\n",
    "\n",
    "We will next need to compute the dot product of all the imput embeddings with this query vector"
   ],
   "id": "3fcb7f4693a981ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T05:22:47.854294Z",
     "start_time": "2025-08-21T05:22:47.847733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = inputs[1]  # X(2)\n",
    "# This same as individually computing the dot product of each input with the query vector,, alternate way is to do inputs @ query\n",
    "attn_scores_2 = torch.matmul(inputs, query)\n",
    "print(f\"Attention scores for X(2): {attn_scores_2}\")\n",
    "# Normalize this vector\n",
    "# attn_weights_2_tmp = attn_scores_2 / torch.sum(attn_scores_2)\n",
    "# print(f\"Normalized attention scores for X(2): {attn_weights_2_tmp}\")\n",
    "\n",
    "#Normalize using softmax\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())\n"
   ],
   "id": "17a1498487a76834",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores for X(2): tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T05:28:56.485345Z",
     "start_time": "2025-08-21T05:28:56.479765Z"
    }
   },
   "cell_type": "markdown",
   "source": "The context vector is weighted sum of these attention weights and the input embesddings",
   "id": "dee7cf664ef2c30c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T05:35:28.535781Z",
     "start_time": "2025-08-21T05:35:28.531243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_vec_2 = torch.sum(\n",
    "    (inputs   # inputs\n",
    "     *\n",
    "    # Reshaping ensures we get the right shape for broadcasting, by having the number of columns as 1 we get a matric is size (N, 1)\n",
    "    # since inputs is of size (N, D), we are able to broadcast each attention weight to D dimensions\n",
    "     attn_weights_2.reshape(-1, 1) ),\n",
    "    dim=0  # By doing dim=0 we aggregate rows retaining the tensor with D columns\n",
    ")\n",
    "print(\"Context vector for X(2):\", context_vec_2)"
   ],
   "id": "4c8662af332af421",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector for X(2): tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T05:35:08.627125Z",
     "start_time": "2025-08-21T05:35:08.625031Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "Conceptually what we did above is shown in the following image\n",
    "\n",
    "![text](./SelfAttention2.png)\n",
    "\n",
    "### We will next compute the attention weights of all input tokens"
   ],
   "id": "2a8a57e85bf1de14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T05:31:57.462636Z",
     "start_time": "2025-08-22T05:31:57.459237Z"
    }
   },
   "cell_type": "markdown",
   "source": "Step 1 is to calculate the attention score of each input token with each other input token. Lets say we have N tokens each of D dimension, the attention score will be of size (N, N) where each row is the attention score of the i<sup>th</sup> token with all other tokens.",
   "id": "24e88f5f8c41d8a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T05:34:16.046869Z",
     "start_time": "2025-08-22T05:34:16.044023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This one liner computes the dot product of each input with every other input\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(f\"Attention scores:\\n{attn_scores}\")\n"
   ],
   "id": "2e81ed08b086de97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores:\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notice how its a symmetric matrix where each row (or even column) is the attention score of the i<sup>th</sup> token with all other tokens.\n",
    "\n",
    "We will now apply softmax normalization to these attn_scores"
   ],
   "id": "adc7204fff13987"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T05:43:38.744537Z",
     "start_time": "2025-08-22T05:43:38.739955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim = 1)\n",
    "print(f\"Attention weights:\\n{attn_weights}\")"
   ],
   "id": "c7f6d223042199e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:\n",
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T05:44:54.506354Z",
     "start_time": "2025-08-22T05:44:54.503435Z"
    }
   },
   "cell_type": "markdown",
   "source": "Finally we will compute the context vector for each input token by multiplying the attention weights with the input embeddings",
   "id": "cfea605c10a20c13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T05:47:53.101704Z",
     "start_time": "2025-08-22T05:47:53.097847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(f\"Context vectors:\\n{all_context_vecs}\")"
   ],
   "id": "5ed76ff160f97dbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors:\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T05:48:17.617872Z",
     "start_time": "2025-08-22T05:48:17.614668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Previous 2nd context vector:\", context_vec_2)\n",
    "print(\"New 2nd context vector:\", all_context_vecs[1])"
   ],
   "id": "80d0d8f5fa6d7bfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n",
      "New 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implement Self Attention with Trainable Weights",
   "id": "7af6faacc173a332"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we see in the following image, we have three sets of weights W<sub>Q</sub>, W<sub>K</sub> and W<sub>V</sub> which are used to transform the input embeddings into Query, Key and Value vectors.\n",
    "\n",
    "![test](./TrainableSelfAttention1.png)"
   ],
   "id": "b1950a9ca3c04f6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will again use the same input x<sub>2</sub> to x<sub>6</sub> as before. Generally the input dimension and output dimensions are same but we will keep it different to illustrate the concept.",
   "id": "dad288c1ff9f6b53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T05:48:36.050568Z",
     "start_time": "2025-08-23T05:48:36.047465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = x_2.shape[0]\n",
    "d_out = 2"
   ],
   "id": "67dbf7072c31c0d6",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T05:52:42.420018Z",
     "start_time": "2025-08-23T05:52:42.416739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ],
   "id": "5dc193a930008a23",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T05:52:42.913595Z",
     "start_time": "2025-08-23T05:52:42.909229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(f\"Query vector for x2: {query_2}\")"
   ],
   "id": "ad4bc5901453a16f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vector for x2: tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T05:54:19.032016Z",
     "start_time": "2025-08-23T05:54:19.029767Z"
    }
   },
   "cell_type": "markdown",
   "source": "We will need the entire key and value matrices to compute the attention weights and context vector, thus we will transform the entire input matrix",
   "id": "533b045060c29670"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T05:55:58.836444Z",
     "start_time": "2025-08-23T05:55:58.833426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n"
   ],
   "id": "3d7e66d362a67cef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "With the query vector and the keys and values metrices, the unscaled attention scores is computed using the dot product of the query vector with each of the key vectors\n",
    "\n",
    "![test](./TrainableSelfAttention2.png)"
   ],
   "id": "48224e9ee61785a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T06:04:17.794098Z",
     "start_time": "2025-08-23T06:04:17.788555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(f\"Attention scores for x2: {attn_scores_2}\")\n",
    "\n",
    "# Notice how the attention scores are scaled by sqrt(d_k) where d_k is the dimension of the key vectors\n",
    "# This is done to avoid very small gradients when d_k is large\n",
    "# See https://arxiv.org/abs/1706.03762 for more details\n",
    "d_k = keys.shape[-1]\n",
    "attn_scores_2 = torch.softmax(attn_scores_2/ d_k**0.5, dim = -1)\n",
    "print(f\"Attention scores for x2: {attn_scores_2}\")\n"
   ],
   "id": "c3bc4a392030b371",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores for x2: tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n",
      "Attention scores for x2: tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![test](./AttentionScalingReason.png)\n",
    "\n",
    "Finally the context vector is computed as the weighted sum of the value vectors"
   ],
   "id": "647fd61fb6867b9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T06:09:39.660458Z",
     "start_time": "2025-08-23T06:09:39.657678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_vec_2 = attn_scores_2 @ values\n",
    "print(f\"Context vector for x2: {context_vec_2}\")"
   ],
   "id": "9de279d145657f80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector for x2: tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![test](./WhyQueryKeyValue.png)",
   "id": "60b97eb6ca23e25b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T18:14:54.985711Z",
     "start_time": "2025-08-23T18:14:54.980907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttentionV1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        return attn_weights @ values"
   ],
   "id": "3521be8276cfdbfd",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T18:14:57.926357Z",
     "start_time": "2025-08-23T18:14:57.920672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttentionV1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ],
   "id": "5700e8a8cea2b0fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "What we see above are the context vectors of each input token after applying self attention. If we notice the second row, it is same as the context vector we computed manually above.",
   "id": "2bef245e630166be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T18:19:11.711364Z",
     "start_time": "2025-08-23T18:19:11.708646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttentionV2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        return attn_weights @ values\n"
   ],
   "id": "eb19e3e0351b27b9",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T18:19:12.301446Z",
     "start_time": "2025-08-23T18:19:12.295752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttentionV2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ],
   "id": "eb9c4e22f4bc845d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "V2 used ``nn.Linear`` insteas of ``nn.Parameter`` to define the weight matrices. This is more standard way of defining the weights in PyTorch.\n",
    "\n",
    "Apart from the weight initialization, the two implementations are identical. Lets verify that by using the weights of the linear layers of V2 to initialize nn.Parameter weights"
   ],
   "id": "447059103327c23d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T18:27:54.128777Z",
     "start_time": "2025-08-23T18:27:54.124999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize nn.Parameter with weights same as V2 weights\n",
    "W_query_test = nn.Parameter(sa_v2.W_query.weight.T)\n",
    "W_key_test = nn.Parameter(sa_v2.W_key.weight.T)\n",
    "W_value_test = nn.Parameter(sa_v2.W_value.weight.T)\n",
    "\n",
    "# perform the attention calculation manually\n",
    "queries_test = inputs @ W_query_test\n",
    "keys_test = inputs @ W_key_test\n",
    "values_test = inputs @ W_value_test\n",
    "attn_scores_test = queries_test @ keys_test.T\n",
    "attn_weights_test = torch.softmax(attn_scores_test / keys_test.shape[-1]**0.5, dim=-1)\n",
    "print(f\"Attention scores calculates using V2 weights manually using nn.Parameter are\\n {attn_weights_test @ values_test}\")"
   ],
   "id": "b829304193ca6c02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores calculates using V2 weights manually using nn.Parameter are\n",
      " tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hiding future words with causal attention\n",
    "\n",
    "For many LLM tasks like text genertion we want to ensure that the model does not attend to future words. This is done using causal attention where we mask the attention scores of future words by setting them to ``-Inf`` before applying softmax."
   ],
   "id": "71fb3ee4236b7d6b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T23:56:51.324604Z",
     "start_time": "2025-08-24T23:56:51.321474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "print(\"Attention scores before masking:\\n\", attn_scores)"
   ],
   "id": "5b8bb40ffd5e1350",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores before masking:\n",
      " tensor([[ 0.2899,  0.0716,  0.0760, -0.0138,  0.1344, -0.0511],\n",
      "        [ 0.4656,  0.1723,  0.1751,  0.0259,  0.1771,  0.0085],\n",
      "        [ 0.4594,  0.1703,  0.1731,  0.0259,  0.1745,  0.0090],\n",
      "        [ 0.2642,  0.1024,  0.1036,  0.0186,  0.0973,  0.0122],\n",
      "        [ 0.2183,  0.0874,  0.0882,  0.0177,  0.0786,  0.0144],\n",
      "        [ 0.3408,  0.1270,  0.1290,  0.0198,  0.1290,  0.0078]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T23:56:41.454670Z",
     "start_time": "2025-08-24T23:56:41.451547Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "We will now mask the upper triangular part of the attention scores by setting them to -Inf. Remember softmax is\n",
    "$$\n",
    "\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
    "$$\n",
    "\n",
    "Setting x to ``-Inf`` to will make $$\\exp(x_i)$$ 0 thus ensuring that the attention weights for those positions are 0\n"
   ],
   "id": "f428993f1c175abf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T00:04:01.211209Z",
     "start_time": "2025-08-25T00:04:01.206097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(\"Masked attention scores:\\n\", masked)\n",
    "attn_weights = torch.softmax(masked / queries.shape[-1]**0.5, dim=-1)\n",
    "print(\"Attention weights after masking:\\n\", attn_weights)\n",
    "context_vec = attn_weights @ values"
   ],
   "id": "2a7bb9a5d75dda8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked attention scores:\n",
      " tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Attention weights after masking:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T00:00:22.877644Z",
     "start_time": "2025-08-25T00:00:22.874098Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Masked attention weights with dropout\n",
    "\n",
    "To prevent overfitting dropout is applied at two possible places\n",
    " 1. After calculating the attention weights\n",
    " 2. After calculating the context vectors\n",
    "\n",
    "Here we will dropout after we calculate the attention weights\n",
    "\n",
    "Lets also illustrate with an example what dropout exactly does"
   ],
   "id": "e85530f67822ba3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T00:09:44.744932Z",
     "start_time": "2025-08-25T00:09:44.733403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)  # 50% dropout\n",
    "example = torch.ones(6, 6)\n",
    "print(f\"Example before dropout:\\n{example}\")\n",
    "dropped = dropout(example)\n",
    "print(f\"Example after dropout:\\n{dropped}\")"
   ],
   "id": "65738ebb193bd632",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example before dropout:\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "Example after dropout:\n",
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "execution_count": 149
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notice how the values of the remaining are scaled by 1/(1-p) where p is the dropout probability. This ensures that the expected value of the tensor remains same before and after dropout. Notice the values dropped doesn't have to exactly half of the values since its a random process. In above example we dropped 15/36 values instead of 18/36 for exact 50% dropout. However the scaling is deterministic and is always done by 1/(1-p)\n",
    "\n",
    "Now lets apply the dropout to the attention weights"
   ],
   "id": "19ac1604d5e7585c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T05:06:27.394304Z",
     "start_time": "2025-08-25T05:06:27.390851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "print(f\"Attention weights with dropout\\n{dropout(attn_weights)}\")"
   ],
   "id": "5f5b24b9a6815ff8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights with dropout\n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 155
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets now implement the self attention with causal masking and dropout. Also lets simulate the batching",
   "id": "dcc409bda8201b46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T05:11:54.978358Z",
     "start_time": "2025-08-25T05:11:54.975821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch = torch.stack([inputs, inputs], dim=0)\n",
    "print(batch.shape)"
   ],
   "id": "d2c5d1791c858a00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T05:27:34.641012Z",
     "start_time": "2025-08-25T05:27:34.633481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # register_buffer ensures that the mask is not a trainable parameter and is moved to the right device when model.to(device) is called\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in_ca = x.shape\n",
    "        queries_ca = self.W_query(x)\n",
    "        keys_ca = self.W_key(x)\n",
    "        values_ca = self.W_value(x)\n",
    "        # Transpose last two dimensions so that we don't transpose the batch size\n",
    "        attn_scores_ca = queries_ca @ keys_ca.transpose(-2, -1)\n",
    "        # The _ indicates that the operation is done in place, also [:num_tokens, :num_tokens]\n",
    "        # ensures that we can handle variable length sequences if there are less than context_length tokens\n",
    "        attn_scores_ca.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights_ca = torch.softmax(attn_scores_ca / d_in_ca ** 0.5, dim=-1)\n",
    "        attn_weights_ca = self.dropout(attn_weights_ca)\n",
    "        return attn_weights_ca @ values_ca\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n"
   ],
   "id": "8543553e3365b752",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "execution_count": 163
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Extending single-head attention to multi-head attention\n",
    "\n",
    " A single causal attention module can be considered single-head attention, where there is only one set of attention weights processing the input sequentially.\n",
    "\n",
    "We will build the intuition of multi-head attention by simply stacking the ``CausalAttention`` modules\n",
    "\n",
    "![test](./MultiheadAttentionConcept.png)\n",
    "\n",
    "The output dimension of the multi-head attention will be ``num_heads * d_out`` since we are concatenating the output of each head"
   ],
   "id": "1ddc113c2f755e0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T04:49:37.104884Z",
     "start_time": "2025-08-26T04:49:37.099182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                        dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Exercise 3.2: Make the dim=-2 instead of dim=-1 and we will concat the context vectors vertically such that dimension of\n",
    "        # of each context vector is preserved however we will double the number of rows in eacxh batch since now we generate\n",
    "        # contect vectors for each token\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "context_vecs = mha(batch)\n",
    "print(f\"context_vecs {context_vecs}\")\n",
    "print(f\"context_vecs.shape: {context_vecs.shape}\")\n"
   ],
   "id": "7b6dddf054d1b25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5866,  0.0071,  0.5869,  0.3214],\n",
      "         [-0.6293, -0.0621,  0.6184,  0.3825],\n",
      "         [-0.5670, -0.0838,  0.5474,  0.3575],\n",
      "         [-0.5519, -0.0979,  0.5319,  0.3423],\n",
      "         [-0.5295, -0.1077,  0.5074,  0.3481]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5866,  0.0071,  0.5869,  0.3214],\n",
      "         [-0.6293, -0.0621,  0.6184,  0.3825],\n",
      "         [-0.5670, -0.0838,  0.5474,  0.3575],\n",
      "         [-0.5519, -0.0979,  0.5319,  0.3423],\n",
      "         [-0.5295, -0.1077,  0.5074,  0.3481]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "execution_count": 179
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Implementing multi-head attention with weight splits\n",
    "\n",
    "What we see above is a naive implementation of multi-head attention where we simply stack multiple single-head attention modules. All these single heads ate processed sequentially which is not efficient."
   ],
   "id": "f7907c83c6e316c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T04:30:01.772965Z",
     "start_time": "2025-08-27T04:30:01.765908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # This is new, we will add an optional Linear layer to project the output.\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in_ca = x.shape\n",
    "        keys_mha = self.W_key(x)       # (b, num_tokens, d_out)\n",
    "        values_mha = self.W_value(x)   # (b, num_tokens, d_out)\n",
    "        queries_mha = self.W_query(x)  # (b, num_tokens, d_out)\n",
    "\n",
    "        # d_out is same as num_heads * head_dim\n",
    "        # view_ reshapes the tensor without changing its data, in this case we project the\n",
    "        # last d_out dimension to (num_heads, head_dim)\n",
    "        keys_mha = keys_mha.view(b, num_tokens, self.num_heads, self.head_dim) # (b, num_tokens, num_heads, head_dim)\n",
    "        values_mha = values_mha.view(b, num_tokens, self.num_heads, self.head_dim) # (b, num_tokens, num_heads, head_dim)\n",
    "        queries_mha = queries_mha.view(b, num_tokens, self.num_heads, self.head_dim) # (b, num_tokens, num_heads, head_dim)\n",
    "\n",
    "        # To calculation the attention score, we need the last two dimensions to be num_tokens and head_dim\n",
    "        # thus we need to transpose the 1st and 2nd dimensions\n",
    "        queries_mha.transpose_(1, 2)  # (b, num_heads, num_tokens, head_dim)\n",
    "        keys_mha.transpose_(1, 2)     # (b, num_heads, num_tokens, head_dim)\n",
    "        values_mha.transpose_(1, 2)   # (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "         # Let calculate the attention scores, this is the dot product of queries and keys\n",
    "        attn_scores_mha = queries_mha @ keys_mha.transpose(-2, -1) # (b, num_heads, num_tokens, num_tokens)\n",
    "\n",
    "        # Apply the mask, the dimensions of the attn scores are still (_, _, context_length, context_length)\n",
    "        #  the mask is 2D and is applied to the last two dimensions only\n",
    "        attn_scores_mha.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) #(b, num_heads, num_tokens, num_tokens)\n",
    "        attn_weights_mha = torch.softmax(attn_scores_mha / self.head_dim ** 0.5, dim=-1) #(b, num_heads, num_tokens, num_tokens)\n",
    "        # Apply dropout to the attention weights\n",
    "        attn_weights_mha = self.dropout(attn_weights_mha) # (b, num_heads, num_tokens, num_tokens)\n",
    "        # attn_weights_mha @ values_mha gives (b, num_heads, num_tokens, head_dim)\n",
    "        # We need to transpose the 1st and 2nd dimensions to get (b, num_tokens, num_heads, head_dim)\n",
    "        context_vecs_mha = (attn_weights_mha @ values_mha).transpose(1,2) # (b, num_tokens, num_heads, head_dim)\n",
    "        # We will reshape the context vectors back to (b, num_tokens, d_out) where d_out = num_heads * head_dim\n",
    "        context_vecs_mha = context_vecs_mha.contiguous().view(b, num_tokens, self.d_out) # (b, num_tokens, d_out)\n",
    "        # Finally we will project the output using the out_proj layer\n",
    "        context_vecs_mha = self.out_proj(context_vecs_mha)\n",
    "        return context_vecs_mha\n",
    "\n"
   ],
   "id": "92fa35d26022f2fe",
   "outputs": [],
   "execution_count": 199
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T04:30:20.411770Z",
     "start_time": "2025-08-27T04:30:20.405555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ],
   "id": "7020dd6a9f1975a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "execution_count": 201
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 3.3\n",
    "\n",
    "Using the MultiHeadAttention class, initialize a multi-head attention module that has the same number of attention heads as the smallest GPT-2 model (12 attention heads). Also ensure that you use the respective input and output embedding sizes similar to GPT-2 (768 dimensions). Note that the smallest GPT-2 model supports a context length of 1,024 tokens."
   ],
   "id": "10600d7fbc4ad971"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T04:34:32.292581Z",
     "start_time": "2025-08-27T04:34:32.277095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_in, d_out = 768, 768\n",
    "context_length = 1024\n",
    "num_heads = 12\n",
    "dropout = 0\n",
    "mha_gpt2 = MultiHeadAttention(d_in, d_out, context_length, dropout, num_heads)\n",
    "print(mha_gpt2)"
   ],
   "id": "36674abd7fde4a04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "  (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "  (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 203
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2edb020574998c58"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
