{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Pretraining on unlabeled data\n",
    "\n",
    "In this notebook we will look at the following\n",
    "\n",
    "![test](./ThisChapter.png)\n",
    "\n",
    "We will take (copy over) the ``GPTModel``(and all other dependencies)  we coded previously in this notebook\n",
    "\n"
   ],
   "id": "d3ce4d451a613ef2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T04:26:25.518997Z",
     "start_time": "2025-09-11T04:26:25.506044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # This is new, we will add an optional Linear layer to project the output.\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in_ca = x.shape\n",
    "        keys_mha = self.W_key(x)       # (b, num_tokens, d_out)\n",
    "        values_mha = self.W_value(x)   # (b, num_tokens, d_out)\n",
    "        queries_mha = self.W_query(x)  # (b, num_tokens, d_out)\n",
    "\n",
    "        # d_out is same as num_heads * head_dim\n",
    "        # view reshapes the tensor without changing its data, in this case we project the\n",
    "        # last d_out dimension to (num_heads, head_dim)\n",
    "        keys_mha = keys_mha.view(b, num_tokens, self.num_heads, self.head_dim) # (b, num_tokens, num_heads, head_dim)\n",
    "        values_mha = values_mha.view(b, num_tokens, self.num_heads, self.head_dim) # (b, num_tokens, num_heads, head_dim)\n",
    "        queries_mha = queries_mha.view(b, num_tokens, self.num_heads, self.head_dim) # (b, num_tokens, num_heads, head_dim)\n",
    "\n",
    "        # To calculation the attention score, we need the last two dimensions to be num_tokens and head_dim\n",
    "        # thus we need to transpose the 1st and 2nd dimensions\n",
    "        queries_mha.transpose_(1, 2)  # (b, num_heads, num_tokens, head_dim)\n",
    "        keys_mha.transpose_(1, 2)     # (b, num_heads, num_tokens, head_dim)\n",
    "        values_mha.transpose_(1, 2)   # (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "         # Let calculate the attention scores, this is the dot product of queries and keys\n",
    "        attn_scores_mha = queries_mha @ keys_mha.transpose(-2, -1) # (b, num_heads, num_tokens, num_tokens)\n",
    "\n",
    "        # Apply the mask, the dimensions of the attn scores are still (b, num_heads, num_tokens, num_tokens)\n",
    "        #  the mask is 2D and is applied to the last two dimensions only\n",
    "        attn_scores_mha.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) #(b, num_heads, num_tokens, num_tokens)\n",
    "        attn_weights_mha = torch.softmax(attn_scores_mha / self.head_dim ** 0.5, dim=-1) #(b, num_heads, num_tokens, num_tokens)\n",
    "        # Apply dropout to the attention weights\n",
    "        attn_weights_mha = self.dropout(attn_weights_mha) # (b, num_heads, num_tokens, num_tokens)\n",
    "        # attn_weights_mha @ values_mha gives (b, num_heads, num_tokens, head_dim)\n",
    "        # We need to transpose the 1st and 2nd (both 0 indexed) dimensions to get (b, num_tokens, num_heads, head_dim)\n",
    "        context_vecs_mha = (attn_weights_mha @ values_mha).transpose(1,2) # (b, num_tokens, num_heads, head_dim)\n",
    "        # We will reshape the context vectors back to (b, num_tokens, d_out) where d_out = num_heads * head_dim\n",
    "        context_vecs_mha = context_vecs_mha.contiguous().view(b, num_tokens, self.d_out) # (b, num_tokens, d_out)\n",
    "        # Finally we will project the output using the out_proj layer\n",
    "        context_vecs_mha = self.out_proj(context_vecs_mha)\n",
    "        return context_vecs_mha\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_batch = torch.mean(x, dim=-1, keepdim=True)\n",
    "        # unbiased=False means we do not use Bessel's correction, that is, we divide by N instead of N-1 (basel's correction)\n",
    "        var_batch = torch.var(x, dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean_batch) / torch.sqrt(var_batch + self.eps)\n",
    "        return norm_x * self.scale + self.shift\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg, hidden_layer_dim_factor = 4):\n",
    "        super().__init__()\n",
    "        emb_dim = cfg[\"emb_dim\"]\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden_layer_dim_factor * emb_dim),\n",
    "            GELU(),\n",
    "            nn.Linear(hidden_layer_dim_factor * emb_dim, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, in_batch):\n",
    "        return self.layers(in_batch)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.dropout_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.dropout_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        return self.out_head(x)"
   ],
   "id": "bedf7560edc7c1b0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Below we will instantiate the GPT model with the configuration of the smallest GPT-2 model (124M parameters), however we will reduce the context length to 256 for faster training. Additionally we will define two methods ``text_to_token_ids`` and ``token_ids_to_text``",
   "id": "7f5be1d8dc72dd1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T04:41:21.540137Z",
     "start_time": "2025-09-11T04:41:20.697278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,  # we will use a smaller context length for faster training, original is 1024\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ],
   "id": "382dd9cda9bebe55",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T04:52:37.138250Z",
     "start_time": "2025-09-11T04:52:36.666811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "def generate_text_simple(model, idx,\n",
    "                          max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Take the context_size tokens to predict the next token\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            logits = model(idx_cond) # (batch_size, context_size, vocab_size)\n",
    "        # Take the last generated token for this is the next token\n",
    "        logits = logits[:, -1, :] # (batch_size, vocab_size)\n",
    "        probs = torch.softmax(logits, dim=-1) # (batch_size, vocab_size)\n",
    "        idx_next = torch.argmax(probs, dim=-1, keepdim=True) # (batch_size, 1)\n",
    "        idx = torch.cat((idx, idx_next), dim=-1) # (batch_size, current_seq_len + 1)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    return torch.tensor(encoded).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    token_ids = token_ids.squeeze(0).tolist()  # Remove batch dimension and convert to list\n",
    "    return tokenizer.decode(token_ids)\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    "    )\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ],
   "id": "46d1ee470d38a5dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T04:45:08.658682Z",
     "start_time": "2025-09-11T04:45:08.656379Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "We can see that the output is not very meaningful, this is because the model is not trained yet. However all our required components are in place.\n",
    "\n",
    "We will next look at loss metric for the generated output\n",
    "\n",
    "### Calculating the text generation loss\n",
    "\n",
    "![test](./TextGenerationSummary.png)\n"
   ],
   "id": "f82a3d839c58b1f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T05:06:44.480847Z",
     "start_time": "2025-09-15T05:06:44.477260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "input_text1 = \"every effort moves\"\n",
    "input_text2 = \"I really like\"\n",
    "\n",
    "expected_output_text1 = \" effort moves you\"\n",
    "expected_output_text2 = \" really like chocolate\"\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "inputs = torch.vstack([text_to_token_ids(input_text1, tokenizer), text_to_token_ids(input_text2, tokenizer)])\n",
    "print(f\"Input token ids are:\\n{inputs}\")\n",
    "\n",
    "targets = torch.vstack([text_to_token_ids(expected_output_text1, tokenizer), text_to_token_ids(expected_output_text2, tokenizer)])\n",
    "print(f\"Output token ids are:\\n{targets}\")\n"
   ],
   "id": "6dfe1980630bc07f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token ids are:\n",
      "tensor([[16833,  3626,  6100],\n",
      "        [   40,  1107,   588]])\n",
      "Output token ids are:\n",
      "tensor([[ 3626,  6100,   345],\n",
      "        [ 1107,   588, 11311]])\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets feed the inputs to the model and get the logits",
   "id": "ac882eb3a43a29e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T05:16:29.709995Z",
     "start_time": "2025-09-15T05:16:29.496599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(f\"Probs shape: {probas.shape}\")"
   ],
   "id": "1012c08f1095a8d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Probs shape: torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There are 2 batches, each with 3 tokens and each token as a probability distribution over the vocabulary of size 50257. What we need is the maximum probability for each of these 3 tokens in 2 batches.\n",
    "\n",
    "Notice how we retain the third dimension by using ``keepdim=True``, if this wasn't provided the the result would have been of shape (2, 3) instead of (2, 3, 1)"
   ],
   "id": "78f9af8f703e3407"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T05:06:46.911294Z",
     "start_time": "2025-09-15T05:06:46.908598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(f\"token_ids are {token_ids} \\n\\nand has shape {token_ids.shape}\")"
   ],
   "id": "4fd7ef63963f13cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids are tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]]) \n",
      "\n",
      "and has shape torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Lets decode and print thiese generated tokens, notice how the generated tokens are not the expected ones. We now need a loss function to measure how far off we are from the expected output. The goal is to increase the softmax probability of the expected output tokens.\n",
    "\n",
    "With a vocabulary size of 50257, the chance probability of getting the correct token is 1/50257 = 0.0000199, this is very low."
   ],
   "id": "51e8af7ddeb087b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T05:06:48.601321Z",
     "start_time": "2025-09-15T05:06:48.596963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ],
   "id": "aad66a8bbb21a534",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Remember target is what we expect the model to output. Lets look at the probabilities of these expected tokens in the generated probabilities.\n",
    "\n",
    "probs has shape (2, 3, 50257) and target has shape (2, 3), our goal while training is to increase the probabilities of these expected tokens relative to other tokens."
   ],
   "id": "d7d285e6ea38bd61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T05:17:01.660003Z",
     "start_time": "2025-09-15T05:17:01.656509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, torch.arange(targets.shape[1]), targets[text_idx]]\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, torch.arange(targets.shape[1]), targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "print(\"Text 2:\", target_probas_2)\n"
   ],
   "id": "e43ae64d4df6d464",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T05:24:11.676500Z",
     "start_time": "2025-09-15T05:24:11.673130Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "We will next calculate the negative log likelihood loss (NLLLoss) which is commonly used for classification problems\n",
    "\n",
    "![Test](./NLL.png)"
   ],
   "id": "bb3785c26be53f86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T05:29:08.830522Z",
     "start_time": "2025-09-15T05:29:08.823422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Flatten and compute the log probabilities\n",
    "log_probas = torch.log(torch.cat([target_probas_1, target_probas_2]))\n",
    "print(log_probas)\n",
    "#Compute the average negative log likelihood loss\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)\n",
    "# We always minimise the loss, thus we take the negative, goal is to make this log_probas as close to 0 as possible\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)\n"
   ],
   "id": "81b46f5ee6cd3aac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n",
      "tensor(-10.7940)\n",
      "tensor(10.7940)\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Pytorch has a built in in ``cross_entropy`` loss function which combines the log softmax and ``NLLLoss`` in one function. We will use this to calculate the loss, recall ``logits`` are the raw outputs of the model before applying softmax\n",
    "\n",
    "We will flatten the first two dimensions of the logits"
   ],
   "id": "fa9444575619f32a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T05:34:12.355538Z",
     "start_time": "2025-09-15T05:34:12.350696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Logits shape is {logits.shape}\")\n",
    "print(f\"Targets shape is {targets.shape}\")\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(\"Cross entropy loss is:\", loss)"
   ],
   "id": "71673dad5fb9ce5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape is torch.Size([2, 3, 50257])\n",
      "Targets shape is torch.Size([2, 3])\n",
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n",
      "Cross entropy loss is: tensor(10.7940)\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e5653f66b23765d9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
